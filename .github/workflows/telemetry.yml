name: Performance Telemetry Collection

on:
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    # Allow manual trigger
  push:
    branches:
      - main
    # Only run on main branch pushes to avoid excessive runs

jobs:
  collect-telemetry:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      actions: read
      issues: write  # For creating regression alerts
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Collect Vercel Analytics
        id: vercel_metrics
        run: |
          if [ -n "${{ secrets.VERCEL_TOKEN }}" ]; then
            # Install Vercel CLI
            npm install -g vercel@latest
            
            # Pull analytics data (if available)
            # Note: This requires Vercel project to be linked
            vercel analytics pull --json > vercel_metrics.json 2>/dev/null || echo '{}' > vercel_metrics.json
            
            # Extract key metrics
            LCP=$(jq -r '.webVitals.LCP // empty' vercel_metrics.json 2>/dev/null || echo '')
            CLS=$(jq -r '.webVitals.CLS // empty' vercel_metrics.json 2>/dev/null || echo '')
            TTFB=$(jq -r '.webVitals.TTFB // empty' vercel_metrics.json 2>/dev/null || echo '')
            ERRORS=$(jq -r '.errors // 0' vercel_metrics.json 2>/dev/null || echo '0')
            
            echo "vercel_metrics<<EOF" >> $GITHUB_OUTPUT
            echo "{\"LCP\": $LCP, \"CLS\": $CLS, \"TTFB\": $TTFB, \"errors\": $ERRORS}" >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          else
            echo "vercel_metrics={}" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true

      - name: Collect GitHub Actions CI Metrics
        id: ci_metrics
        run: |
          if [ -n "${{ secrets.GITHUB_TOKEN }}" ]; then
            # Get recent workflow runs
            OWNER=$(echo "${{ github.repository }}" | cut -d'/' -f1)
            REPO=$(echo "${{ github.repository }}" | cut -d'/' -f2)
            
            # Fetch last 10 workflow runs
            RUNS=$(curl -s \
              -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
              -H "Accept: application/vnd.github.v3+json" \
              "https://api.github.com/repos/${OWNER}/${REPO}/actions/runs?per_page=10" \
              | jq '.workflow_runs[] | {id, status, conclusion, created_at, updated_at}' \
              | jq -s '.')
            
            # Calculate metrics
            TOTAL=$(echo "$RUNS" | jq 'length')
            SUCCESS=$(echo "$RUNS" | jq '[.[] | select(.conclusion == "success")] | length')
            FAILURE=$(echo "$RUNS" | jq '[.[] | select(.conclusion == "failure")] | length')
            FAILURE_RATE=$(echo "scale=2; $FAILURE / $TOTAL * 100" | bc -l 2>/dev/null || echo "0")
            
            # Calculate average build duration (in minutes)
            AVG_DURATION=$(echo "$RUNS" | jq -r '
              [.[] | 
                (.updated_at | fromdateiso8601) - (.created_at | fromdateiso8601)
              ] | 
              if length > 0 then add / length / 60 else 0 end' 2>/dev/null || echo "0")
            
            # Count pending/running workflows
            QUEUE_LENGTH=$(curl -s \
              -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
              -H "Accept: application/vnd.github.v3+json" \
              "https://api.github.com/repos/${OWNER}/${REPO}/actions/runs?status=in_progress&per_page=10" \
              | jq '.workflow_runs | length' 2>/dev/null || echo "0")
            
            echo "ci_metrics<<EOF" >> $GITHUB_OUTPUT
            echo "{\"buildDurationMin\": $AVG_DURATION, \"failureRate\": $FAILURE_RATE, \"queueLength\": $QUEUE_LENGTH}" >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          else
            echo "ci_metrics={}" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true

      - name: Collect Expo Build Metrics
        id: expo_metrics
        run: |
          if [ -n "${{ secrets.EXPO_TOKEN }}" ]; then
            # Install EAS CLI
            npm install -g eas-cli@latest
            
            # Get recent builds
            eas build:list --json --limit=10 > expo_builds.json 2>/dev/null || echo '[]' > expo_builds.json
            
            # Calculate metrics
            TOTAL=$(jq 'length' expo_builds.json 2>/dev/null || echo "0")
            SUCCESS=$(jq '[.[] | select(.status == "finished")] | length' expo_builds.json 2>/dev/null || echo "0")
            SUCCESS_RATE=$(echo "scale=2; $SUCCESS / $TOTAL * 100" | bc -l 2>/dev/null || echo "0")
            
            # Average bundle size (if available)
            BUNDLE_MB=$(jq -r '[.[] | .artifacts.buildUrl // empty] | length' expo_builds.json 2>/dev/null || echo "0")
            
            # Average build duration
            AVG_DURATION=$(jq -r '
              [.[] | 
                (.completedAt | fromdateiso8601) - (.createdAt | fromdateiso8601)
              ] | 
              if length > 0 then add / length / 60 else 0 end' expo_builds.json 2>/dev/null || echo "0")
            
            echo "expo_metrics<<EOF" >> $GITHUB_OUTPUT
            echo "{\"bundleMB\": $BUNDLE_MB, \"buildDurationMin\": $AVG_DURATION, \"successRate\": $SUCCESS_RATE}" >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          else
            echo "expo_metrics={}" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true

      - name: Store Metrics to Supabase
        run: |
          python3 << 'EOF'
          import os
          import json
          import requests
          from datetime import datetime
          
          supabase_url = os.environ.get('NEXT_PUBLIC_SUPABASE_URL')
          supabase_key = os.environ.get('SUPABASE_SERVICE_ROLE_KEY')
          
          if not supabase_url or not supabase_key:
              print("Supabase credentials not configured")
              exit(0)
          
          # Read metrics from GitHub outputs
          vercel_metrics = json.loads(os.environ.get('VERCEL_METRICS', '{}'))
          ci_metrics = json.loads(os.environ.get('CI_METRICS', '{}'))
          expo_metrics = json.loads(os.environ.get('EXPO_METRICS', '{}'))
          
          # Store each metric source
          sources = [
              ('vercel', vercel_metrics),
              ('ci', ci_metrics),
              ('expo', expo_metrics),
          ]
          
          for source, metric_data in sources:
              if metric_data:
                  payload = {
                      'source': source,
                      'metric': metric_data
                  }
                  
                  response = requests.post(
                      f'{supabase_url}/rest/v1/metrics_log',
                      headers={
                          'apikey': supabase_key,
                          'Authorization': f'Bearer {supabase_key}',
                          'Content-Type': 'application/json',
                          'Prefer': 'return=minimal'
                      },
                      json=payload
                  )
                  
                  if response.status_code in [200, 201]:
                      print(f"âœ“ Stored {source} metrics")
                  else:
                      print(f"âœ— Failed to store {source} metrics: {response.status_code}")
                      print(response.text)
          EOF
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          VERCEL_METRICS: ${{ steps.vercel_metrics.outputs.vercel_metrics }}
          CI_METRICS: ${{ steps.ci_metrics.outputs.ci_metrics }}
          EXPO_METRICS: ${{ steps.expo_metrics.outputs.expo_metrics }}
        continue-on-error: true

      - name: Analyze for Regressions
        id: regression_check
        run: |
          python3 << 'EOF'
          import os
          import json
          import requests
          
          supabase_url = os.environ.get('NEXT_PUBLIC_SUPABASE_URL')
          supabase_key = os.environ.get('SUPABASE_SERVICE_ROLE_KEY')
          
          if not supabase_url or not supabase_key:
              print("Supabase credentials not configured")
              exit(0)
          
          # Fetch last 2 days of metrics
          response = requests.get(
              f'{supabase_url}/rest/v1/metrics_log',
              headers={
                  'apikey': supabase_key,
                  'Authorization': f'Bearer {supabase_key}',
              },
              params={
                  'source': 'eq.vercel',
                  'order': 'ts.desc',
                  'limit': '100'
              }
          )
          
          if response.status_code != 200:
              print("Failed to fetch metrics")
              exit(0)
          
          metrics = response.json()
          
          # Compare recent vs previous period
          if len(metrics) < 10:
              print("Not enough metrics for regression analysis")
              exit(0)
          
          # Simple regression detection
          recent_lcp = [m['metric'].get('LCP', 0) for m in metrics[:5] if m['metric'].get('LCP')]
          previous_lcp = [m['metric'].get('LCP', 0) for m in metrics[5:10] if m['metric'].get('LCP')]
          
          if recent_lcp and previous_lcp:
              recent_avg = sum(recent_lcp) / len(recent_lcp)
              previous_avg = sum(previous_lcp) / len(previous_lcp)
              
              if previous_avg > 0:
                  change_pct = ((recent_avg - previous_avg) / previous_avg) * 100
                  
                  if change_pct > 10:  # 10% regression threshold
                      print(f"REGRESSION_DETECTED=true" >> os.environ.get('GITHUB_OUTPUT', '/dev/stdout'))
                      print(f"REGRESSION_METRIC=LCP" >> os.environ.get('GITHUB_OUTPUT', '/dev/stdout'))
                      print(f"REGRESSION_CHANGE={change_pct:.1f}" >> os.environ.get('GITHUB_OUTPUT', '/dev/stdout'))
                      print(f"âš ï¸ Performance regression detected: LCP increased by {change_pct:.1f}%")
          EOF
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        continue-on-error: true

      - name: Create Regression Alert
        if: steps.regression_check.outputs.REGRESSION_DETECTED == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const metric = '${{ steps.regression_check.outputs.REGRESSION_METRIC }}';
            const change = '${{ steps.regression_check.outputs.REGRESSION_CHANGE }}';
            
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `ðŸš¨ Performance Regression Detected: ${metric} increased by ${change}%`,
              body: `## Performance Regression Alert
              
              **Metric:** ${metric}
              **Change:** +${change}%
              **Threshold:** >10% increase
              
              This regression was automatically detected by the Performance Intelligence Layer.
              
              ### Recommended Actions:
              - Review recent deployments
              - Check for new dependencies or code changes
              - Enable image compression if LCP is affected
              - Review Supabase query performance if TTFB is affected
              
              ### View Dashboard:
              Check the performance dashboard at \`/admin/metrics\` for detailed insights.
              
              ---
              *This issue was automatically created by the Performance Telemetry workflow.*
              `,
              labels: ['performance', 'regression', 'automated']
            });

      - name: Generate Performance Report
        run: |
          python3 scripts/generate_performance_report.py || echo "Report generation skipped"
        continue-on-error: true
